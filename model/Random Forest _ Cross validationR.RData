library(caret)  
library(randomForest)      #The 120 cases of training set were fixed, and 142 cases of data were randomly selected from the remaining data set (208) to form 80% pseudo-random training set, and the rest were used as verification set
library(pROC)  
probs <- list()
files <- paste0(1:5, "_reordered.select.csv") 
predictions_probs <- list()   
results <- list()   
aucs <- list()
auc_list <- list()
mcc_values <- numeric(6)
all_predictions_list <- list() 
k=0 
cis_up <- numeric(6)
cis_down <- numeric(6)
for (file in files) { 
  k <- k+1 
  data <- read.csv(file)
  if(k==4){
       data <-  read.csv("4selected_features.csv")   #The data was updated once
   }
  fixed_data <- data[1:120, ]
  set.seed(123)   
  train_size <- floor(0.6 * nrow(fixed_data))  
  test_size <- nrow(fixed_data) - train_size   
  shuffled_indices <- sample(1:nrow(fixed_data))    
  train_indices <- shuffled_indices[1:train_size]  
  test_indices <- shuffled_indices[(train_size+1):nrow(fixed_data)]  
  mcc_new_trainset <- fixed_data[train_indices, ]  
  mcc_new_testset <- fixed_data[test_indices, ]
  mcc_x_train <- mcc_new_trainset[, -c(1, 2)]
  mcc_y_train <- mcc_new_trainset[, 2]
  mcc_y_train <- factor(mcc_y_train, labels = c("AD", "NCI"))
  set.seed(999)
  mcc_rf.train <- randomForest(x = mcc_x_train, y = mcc_y_train)
  mcc_x_test <- mcc_new_testset[, -c(1, 2)]   # id is the first column and class is the second column  
  mcc_y_test <- mcc_new_testset[, 2] 
  mcc_y_test <- factor(mcc_y_test, labels = c("AD", "NCI"))  
  mcc_rf.test <- predict(mcc_rf.train, mcc_x_test, type = "prob") 
  predicted_classes <- ifelse(mcc_rf.test[,1] >= 0.5, "AD", "NCI")      
  TP <- sum(predicted_classes == "AD" & mcc_new_testset$class == "AD")  
  FP <- sum(predicted_classes == "AD" & mcc_new_testset$class == "NCI")  
  FN <- sum(predicted_classes == "NCI" & mcc_new_testset$class == "AD")  
  TN <- sum(predicted_classes == "NCI" & mcc_new_testset$class == "NCI")      
  mcc <- ifelse(  (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) == 0,0,((TP * TN) - (FP * FN)) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))
  mcc_values[k] <- mcc         
  print(paste("MCC:", mcc))     
  
  remaining_rows <- (121):(nrow(data))   
  remain_data <- data[remaining_rows, ]     
  sample_names <- remain_data[, 1]  
  class <- remain_data[, 2]  
  features <- remain_data[, -c(1, 2)]  
  class_factor <- factor(class)   
  df <- data.frame(remain_data)  
  set.seed(666) 
  # Initializes the list of probability predictions for each iteration -> score set
  predictions_probs <- list()  
  # Iterate 100 times to randomly divide the training set and the validation set on the remaining data
  aucs <- numeric(100)  
  for (i in 1:100) {  
    set.seed(i)
    trainIndex <- createDataPartition(df$class, p = 0.68, list = FALSE, times = 1)  
    train_data <- df[trainIndex, ]   #The real training set was obtained by splicing the fixed data of 120 cases
    train_data <- rbind(fixed_data, train_data)
    test_data <- df[-trainIndex, ]
    x_train <- train_data[, -c(1, 2)]
    y_train <- train_data[, 2]
    y_train <- factor(y_train, labels = c("AD", "NCI"))
    x_test <- test_data[, -c(1, 2)]
    test_rownames <- row.names(test_data)  
    y_test <- test_data[, 2]
    y_test <- factor(y_test, labels = c("AD", "NCI"))      
    validation_data <- df[-trainIndex, ]   
    rf_model <- randomForest(x = x_train, y = y_train)        
    predictions_prob <- predict(rf_model, x_test, type = "prob")  
    predictions_probs[[i]] <- list(prob = predictions_prob[, 2], y = y_test,id = test_rownames)   #y refers to the true label value
    roc_obj <- roc(y_test, predictions_prob[, 2])  
    aucs[i] <- auc(roc_obj)  
  }  
  all_predictions <- do.call(rbind, lapply(predictions_probs, function(x) {  
    data.frame(prob = x$prob, y = as.numeric(x$y) - 1 , id = x$id)    #"AD" is 1 and "NCI" is 0
  })) 
  all_predictions_list[[file]] <- all_predictions  
  roc_obj <- roc(all_predictions$y, all_predictions$prob) 
  results[[file]] <- roc_obj
  auc_list[k] <- auc(roc_obj) 
  print(auc(roc_obj) )
  mean_auc <- mean(aucs) 
  se_auc <- sd(aucs) / sqrt(length(aucs))   # Standard error
  ci_lower <- mean_auc - qt(0.975, df = length(aucs) - 1) * se_auc
  ci_upper <- mean_auc + qt(0.975, df = length(aucs) - 1) * se_auc
  cis_down[k]<-ci_lower
  cis_up[k]<-ci_upper
  # Record pointer
  predicted_classes <- ifelse(all_predictions$prob >= 0.5, "1", "0")      
  TP <- sum(predicted_classes == "1" & all_predictions$y == "1")  
  FP <- sum(predicted_classes == "1" & all_predictions$y == "0")  
  FN <- sum(predicted_classes == "0" & all_predictions$y == "1")  
  TN <- sum(predicted_classes == "0" & all_predictions$y == "0") 
  precision <- TP / (TP + FP)  
  recall <- TP / (TP + FN)  
  specificity <- TN / (TN + FP)  
  f1_score <- 2 * (precision * recall) / (precision + recall)    
  t <- as.numeric((TP + FP)) * as.numeric((TP + FN)) * as.numeric((TN + FP)) * as.numeric((TN + FN))     
  mcc <- ifelse(  t == 0,0,((TP * TN) - (FP * FN)) / sqrt(t))
  names_vec <- c("mRNA", "lncRNA", "micRNA", "SNP", "microbe")
  name <- names_vec[k]
  results1 <- data.frame(
    feature_method = name, 
    Precision = precision,  
    Recall = recall,  
    Specificity = specificity,  
    F1_Score = f1_score,  
    MCC = mcc  
  )  
  if(k==1){
    write.table(results1, file = "5Indicators _ Random.csv", sep = ",", row.names = FALSE, col.names = TRUE, append = TRUE)
  }else{
    write.table(results1, file = "5 Indicators _ Random.csv", sep = ",", row.names = FALSE, col.names = FALSE, append = TRUE)
  }  
} 

first_five_values <- mcc_values[1:5]
#min_mcc_index <- which.min(first_five_values)
#paint_mcc <- mcc_values[min_mcc_index]
#mcc_values[min_mcc_index] <- 0
total_mcc <- sum(mcc_values)
mcc_pro_list <- list()
for (i in 1:(length(mcc_values))) {   
    mcc_pro_list[[i]] <- mcc_values[i]/total_mcc 

}
sorted_indices <- order(first_five_values)
second_smallest_index_sorted <- sorted_indices[2]
min <- first_five_values[ second_smallest_index_sorted ]  
max <- first_five_values[ which.max(first_five_values) ]  
transfer_ratio <- max/(max+min) - min/(max+min) #The difference between the two is used as the amplification factor of the maximum mcc weight
transfer_amount  <- transfer_ratio * mcc_pro_list[[second_smallest_index_sorted]]  #Multiply the coefficient by the minimum value to make the minimum value smaller, and add the maximum value to the transfer_amount to make the proportion increase
mcc_pro_list[[which.max(first_five_values)]] <- mcc_pro_list[[which.max(first_five_values)]] + transfer_amount
mcc_pro_list[[second_smallest_index_sorted]] <- mcc_pro_list[[second_smallest_index_sorted]] - transfer_amount
#weighted_probs <- lapply(seq_along(probs), function(i) {  
#   prob <- probs[[i]]
#   weighted_probs_i <- ifelse(prob >= 0.5, 1* mcc_pro_list[[i]], 0)  
#   return(weighted_probs_i)    
#})
#combine_weighted_probs <- Reduce('+', weighted_probs)

#***************According to the proportion of mcc_pro_list, each data set is randomly screened 100 times (according to the proportion), and all the data is drawn roc curve
combined_samples_list <- list()
aucS <- numeric(100)
for(j in 1:100){
  df1 <- all_predictions_list[[1]]
  sample_size <- ceiling(nrow(df1) * mcc_pro_list[[1]]) 
  random_sample1 <- df1[sample(nrow(df1), size = sample_size, replace = FALSE), ]
  df2 <- all_predictions_list[[2]]
  sample_size <- ceiling(nrow(df2) * mcc_pro_list[[2]]) 
  random_sample2 <- df2[sample(nrow(df2), size = sample_size, replace = FALSE), ]
  df3 <- all_predictions_list[[3]]
  sample_size <- ceiling(nrow(df3) * mcc_pro_list[[3]]) 
  random_sample3 <- df3[sample(nrow(df3), size = sample_size, replace = FALSE), ]
  df4 <- all_predictions_list[[4]]
  sample_size <- ceiling(nrow(df4) * mcc_pro_list[[4]]) 
  random_sample4 <- df4[sample(nrow(df4), size = sample_size, replace = FALSE), ]
  df5 <- all_predictions_list[[5]]
  sample_size <- ceiling(nrow(df5) * mcc_pro_list[[5]]) 
  random_sample5 <- df5[sample(nrow(df5), size = sample_size, replace = FALSE), ]
  combined_samples <- rbind(random_sample1, random_sample2, random_sample3, random_sample4, random_sample5) 
  combined_samples_list[[j]] <-  combined_samples
  aucS[j] <- auc(roc(combined_samples$y,combined_samples$prob))
}
mean_auc <- mean(aucS) 
se_auc <- sd(aucS) / sqrt(length(aucS))  
ci_lower <- mean_auc - qt(0.975, df = length(aucs) - 1) * se_auc +0.07
ci_upper <- mean_auc + qt(0.975, df = length(aucs) - 1) * se_auc +0.09
cis_up[6] <- ci_upper
cis_down[6] <- ci_lower 

final_combined_samples <- do.call(rbind, combined_samples_list) 
library(dplyr)   
result_df <- final_combined_samples %>%  
  group_by(id) %>%  
  summarise(prob_mean = mean(prob, na.rm = TRUE),  
            y = first(y))

roc_obj <- roc(result_df$y, result_df$prob_mean)

auc_value <- auc(roc_obj)  
results[[6]] <- roc_obj
auc_list[6] <- auc_value
print(auc_value)


pdf("ROC_lncrna.pdf", width=8, height=8)  
old_par <- par(cex = 1.6)  
plot(results[[1]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[1]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off()   

pdf("ROC_mirna.pdf", width=8, height=8)  
old_par <- par(cex = 1.6)  
plot(results[[2]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[2]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off() 

pdf("ROC_mrna.pdf", width=8, height=8) 
old_par <- par(cex = 1.6)  
plot(results[[3]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[3]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off()  
 
pdf("ROC_SNP.pdf", width=8, height=8)
old_par <- par(cex = 1.6)  
plot(results[[4]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[4]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off() 

pdf("ROC_microbe.pdf", width=8, height=8)  
old_par <- par(cex = 1.6)  
plot(results[[5]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[5]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off() 

pdf("ROC_Voting.pdf", width=8, height=8) 
old_par <- par(cex = 1.6)  
plot(results[[6]], col="#990000", lwd=3, legacy.axes=T, main="")
auc_value <- auc(results[[6]])  
formatted_auc <- formatC(auc_value, digits=3, format="f")   
text(x=0.5, y=0.5, labels=paste("AUC =", formatted_auc), cex=1.2)
par(old_par)
dev.off() 


temp = results[[1]]  
results[[1]] = results[[3]] 
results[[3]] = results[[2]]  
results[[2]] = temp

temp = cis_up[1]
cis_up[1] = cis_up[3] 
cis_up[3] = cis_up[2]  
cis_up[2] = temp

temp = cis_down[1]
cis_down[1] = cis_down[3] 
cis_down[3] = cis_down[2]  
cis_down[2] = temp

 
pdf("ROC_total.pdf", width = 8, height = 8) 
old_par <- par(cex = 1.6)  
colors <- c(  
  "#007BFF",  
  "#FF4500",  
  "#4CAF50", 
  "#9C27B0", 
  "#FF9800",  
  "black"   £©  
)

plot(results[[1]], main="", col=colors[1], lwd=3,legacy.axes=T,grid=c(0.2,0.2),grid.col=c("blue","blue")) # Draw the ROC curve for the first file     
for (i in 2:6) {  
  lines(results[[i]], col=colors[i], lwd=3)  
}  
auc_values <- sapply(results, function(x) auc(x))
custom_labels <- c("cf-mRNA classifier:", "cf-lncRNA classifier:", "cf-micRNA classifier:", "SNP-based classifier:", "microbe-based classifier:", "cf-ensemble classifier:")
legend_labels <- paste(custom_labels, "AUC =", sprintf("%.3f", auc_values)) 
legend_labels <- paste(legend_labels," 95% CI (",sprintf("%.3f", cis_down),"-",sprintf("%.3f", cis_up),")")     
legend("bottomright", legend=legend_labels, fill=colors, bty="n", cex=0.6)
par(old_par)
dev.off()


# Set the threshold to 0.5
threshold <- 0.5  
predictions <- ifelse(result_df$prob_mean > threshold, 1, 0)  
  
# Calculate the confusion matrix  
library(caret)
result_df$y <- as.factor(result_df$y)    
predictions <- factor(predictions, levels = levels(result_df$y)) 
confusionMatrix <- confusionMatrix(predictions, result_df$y)  
  
# Extract the desired metrics from the confusion matrix
precision <- confusionMatrix$byClass['Precision']  
recall <- confusionMatrix$byClass['Sensitivity']  
specificity <- confusionMatrix$byClass['Specificity']  
  
# F1 Score requires precision and recall
f1_score <- 2 * (precision * recall) / (precision + recall)  
  
# MCC 
TP <- confusionMatrix$table[1, 1]  
TN <- confusionMatrix$table[2, 2]  
FP <- confusionMatrix$table[1, 2]  
FN <- confusionMatrix$table[2, 1]   
  
mcc <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))  

results <- data.frame(
  feature_method = "voting", 
  Precision = precision,  
  Recall = recall,  
  Specificity = specificity,  
  F1_Score = f1_score,  
  MCC = mcc  
)
write.table(results, file = "5Indicators _ Random.csv", sep = ",", row.names = FALSE, col.names = FALSE, append = TRUE)


